[ { "title": "Openstack Deployment - Hardware", "url": "/posts/openstack-deployment-hardware/", "categories": "Training-SOC, OpenStack", "tags": "openstack, cloud, deployment, soc, guide, juju, maas", "date": "2021-07-26 08:00:00 -0600", "snippet": "ConfigurationDell PowerEdge ServersBSU’s SOC environment consists of Dell PowerEdge servers (no iDRAC license), so set-up steps are mostly identical across servers. Connect a display and keyboard (and mouse if preferred) to the server. Power on the server and wait until the boot menu on the top of the screen is displayed - at this point press F10 (Life Cycle Manager) on the keyboard and wait for the screen to load. Once in Life Cycle Manager we need to set up the following:RAID ControllerFor setting up the raid controller, network storage and local storage have different virtual disk layouts. Local storage Local storage will need virtual disks created. The first virtual disk will be comprised of two drives, and will be the Here, select “Network”.Since we do not have an iDRAC license we will use NIC1 for iDRAC instead of the dedicated iDRAC NIC. In our case we did not specify a “Fallover Network”.Scrolling down, there are settings for registering iDRAC on DNS, which we disabled. We enabled DHCP for IPv4 as DHCP makes enlisting nodes on MAAS much easier, and we also enabled using DHCP to obtain DNS addresses.Lastly, and the whole point of setting up the iDRAC in the first place - we need to enable IPMI so that MAAS can power-cycle the machines. Below are the default settings - you can choose to assign a different privilege or encryption key but keep in mind that if you change the default, to make IPMI work you will need to specify both of these parameters in MAAS when setting up a new node.Now press the ESC key to return to the Life Cycle Manager main page and you will be asked to reboot. At this point you are done setting up iDRAC + IPMI settings.3x RAID Virtual DisksThe first virtual drive is set up using the Wizard on the “Home” tab of Life Cycle Manager. This first step is ideal if the drives have foreign raid configurations already (if the drives came from another server). The wizard will walk you through the RAID configuration. Note: This is just one of three virtual disks we will set up so allocate physical drives accordingly.The subsequent two virtual disks will be set up using the device manager itself. To access this, go to the “System Setup” tab of Life Cycle Manager and select “Advanced Hardware Configuration”. From there select Device Settings and then select the RAID controller from the list of devices. Now press the ESC key to return to the Life Cycle Manager main page and you will be asked to reboot. At this point you are done setting up your three RAID virtual drives.PXE BootWhen MAAS wakes a server using IPMI, the server needs to perform a PXE Network boot by default. To enable this, go to the “System Setup” tab of Life Cycle Manager and select “Advanced Hardware Configuration”.From here, select “System BIOS”, and once in System BIOS ensure the Boot Mode is set to UEFI. During our testing phase we also disabled Boot Sequence retry for debugging purposes. Next, click on “UEFI Boot Settings”.Once in UEFI boot settings select “UEFI Boot Sequence ‘’, and use the +/- keys to move the NIC that you will be PXE booting from to the top of the list. In our case, we PXE boot from the same NIC that we perform IPMI over. Now press the ESC key to return to the Life Cycle Manager main page and you will be asked to reboot. At this point you are done setting up PXE boot settings.Now press the ESC key to return to the Life Cycle Manager main page and you will be asked to reboot. At this point you are done setting up PXE boot settings.Next Section: Network" }, { "title": "OpenStack Deployment - Overview", "url": "/posts/openstack-deployment-overview/", "categories": "Training-SOC, OpenStack", "tags": "openstack, cloud, deployment, soc, guide, charms, juju, maas", "date": "2021-01-30 07:00:00 -0700", "snippet": "The purpose of this guide is to aid in the replication and maintenance of our OpenStack cloud by documenting the deployment process.VersionsThe software versions used in the BSU environment are as follows: Ubuntu 20.04 LTS (Focal) for the MAAS server, Juju client, Juju controller, and all cloud nodes (including containers) MAAS 2.9.2 Juju 2.7.6 OpenStack WallabyHardware Layout (5 Nodes) Name Description Client Machine Machine used to SSH into the MAAS Controller, or MAAS nodes. MAAS Controller This machine is not part of the OpenStack cloud - it simply manages the “metal” of the cloud by power-cycling machines and network booting and deploying OS images. JuJu Controller One of the nodes managed by MAAS. Not to be confused with the JuJu application that lives within the MAAS Controller. The JuJu Controller orchestrates the commands given to the JuJu application and hosts the JuJu dashboard. node0-3 These MAAS managed nodes (tagged “compute”) are where the OpenStack Charm containers get deployed to. These nodes form the actual OpenStack cloud. Deployment ProcessThe first step in deployment is to configure the hardware (servers) so they can play nicely with the MAAS Application later in the deployment process. It is important to ensure all minimal hardware requirements are met.Next, there needs to be some thought put into network architecture, and the Network Set-Up section lists some of the considerations.With hardware and network configuration done, Ubuntu Server 20.04 LTS needs to be installed on a server separate from the OpenStack cluster - this server is referred to as the “MAAS Controller”. The MAAS Application is then installed on the MAAS Controller using the Snap Store, and is then configured appropriately.The servers that are part of the OpenStack cluster (which are referred to as JuJu Controller, Node0, Node1, Node2, and Node3) need to then be enlisted on MAAS, and set to a “ready” state to be “deployed” later on by JuJu. This process is covered in the MAAS Enlisting section.Before any JuJu magic can happen, the JuJu Application needs to be installed on the MAAS Controller using the Snap Store - all JuJu commands will be run on the MAAS Controller command line, but will be orchestrated by the JuJu Controller.To actually deploy the JuJu Controller hardware (and nodes later on), the JuJu Application needs to have API access to MAAS. This quick set-up process is outlined in the JuJu Configuration section. And once JuJu can control MAAS-managed hardware through API, the actual JuJu controller can be bootstrapped and deployed - this is done through command line on the MAAS Controller as described in the JuJu Deployment section.With the JuJu Controller deployed, the OpenStack installation can begin. This (lengthy and tedious) process is described step-by-step under “Open Stack Installation”. The OpenStack cloud then needs to be configured for non-admin access and use, and managed for maintenance, scalability and power events - all of which is outlined in detail in the OpenStack Management section.Next Section: Hardware" }, { "title": "OpenStack Deployment - Network", "url": "/posts/openstack-deployment-network/", "categories": "Training-SOC, OpenStack", "tags": "openstack, cloud, deployment, soc, guide, charms, juju, maas", "date": "2021-01-28 07:00:00 -0700", "snippet": "ArchitectureServing DHCPWe have the option to let our router or MAAS serve DHCP, and both options have their implications.Letting MAAS Serve DHCPWhen MAAS is serving DHCP, MAAS will automatically discover a server that is PXE booting on the same network. When a server is booting over PXE, MAAS takes over by providing a boot image and automatically listing the node under “Machines” on the MAAS Application GUI - and if IPMI was set up correctly, MAAS will automatically configure the IPMI. Though this is very convenient, the downside however, is that the Firewall/Router won’t manage DHCP leases.Letting the Firewall/Router Serve DHCPAlternatively, we can let the router serve DHCP. The main consideration here (as far as MAAS is concerned) is that we will have to manually enlist new servers by MAC and IP address, which isn’t a super tedious process but it is something to consider.Next Section: MAAS" }, { "title": "OpenStack Deployment - MAAS", "url": "/posts/openstack-deployment-maas/", "categories": "Training-SOC, OpenStack", "tags": "openstack, cloud, deployment, soc, guide, charms, juju, maas", "date": "2021-01-27 07:00:00 -0700", "snippet": "Controller OSOne server that isn’t part of the OpenStack cluster (but is on the same network) needs to be the designated “MAAS Controller”. The MAAS Controller will “manage” the hardware (servers), meaning it can power on/off + deploy operating system images to each of the servers networked in the OpenStack cluster.Installing the Operating SystemThough most Debian Linux distributions would work, our MAAS Controller runs Ubuntu Server Focal 20.04 LTS.A Few Pointers For troubleshooting purposes, it is advisable to manually assign an IP during the installation ofUbuntu rather than automatically using DHCP. The reason why, is if the DHCP service within MAAS stops working, the MAAS controller won’t have an IP, and you won’t be able to SSH into it - and that brings up the next point below. It is infinitely easier to manage the MAAS Controller over SSH, however, Ubuntu Server comes out of the box with UFW, so SSH needs to be allowed through the firewall. To allow ssh, input the following command. sudo ufw allow ssh InstallationWith Ubuntu configured on the MAAS Controller, the MAAS application needs to be installed using the Snap Store by inputting this command:sudo snap install --channel=2.9/stable maasMAAS needs a database backend, which can be installed using the following commands.sudo apt update -ysudo apt install -y postgresqlA user for the database then needs to be created. Input the following command - the $VARIABLE’s need to be replaced appropriately and documented for future reference.sudo -u postgres psql -c &quot;CREATE USER \\&quot;$MAAS_DBUSER\\&quot; WITH ENCRYPTED PASSWORD &#39;$MAAS_DBPASS&#39;&quot;Then the database needs to be created with the user created above. Run the following command replacing the $VARIABLE’s.sudo -u postgres createdb -O &quot;$MAAS_DBUSER&quot; &quot;$MAAS_DBNAME&quot;A postgres config file now needs to be edited to reflect the new database user and database name. At the time of writing this guide, the latest stable version of postgres is version 12, so the following command will use vim to open the file in this directory - /etc/postgresql/12/main/pg_hba.conf using root privilege. In the event this doesn’t work, manually navigate to this directory, you likely have a different version of postgres installed so the ‘12’ in the directory may be a different number.sudo vim /etc/postgresql/12/main/pg_hba.confWith the pg_hba.conf file open, add the following line at the end, and save the file.host $MAAS_DBNAME $MAAS_DBUSER 0/0 md5Now it is time to initialize the MAAS application using the command below. Replace the variables as appropriate, and since we are running the MAAS application on our MAAS Controller, the $HOSTNAME should be set to localhost.sudo maas init region+rack --database-uri &quot;postgres://$MAAS_DBUSER:$MAAS_DBPASS@$HOSTNAME/$MAAS_DBNAME&quot;Note that the IP of the web GUI is printed out on the terminal during this step. Press enter unless you want to make any changes. If you specified that the $HOSTNAME was localhost, the IP of the web GUI is the same as that of the MAAS Controller machine.Finally, a MAAS admin user needs to be created.sudo maas createadminYou will be prompted to input user credentials - these will be used to log into the web GUI so make sure to document these for future use.Now check the status of the MAAS application using the following commandsudo maas statusThe output should be similar to the following.sudo maas statusbind9 RUNNING pid 22236, uptime 0:01:02dhcpd STOPPED Not starteddhcpd6 STOPPED Not startedhttp RUNNING pid 22461, uptime 0:00:42ntp RUNNING pid 22372, uptime 0:00:45proxy RUNNING pid 22604, uptime 0:00:34rackd RUNNING pid 22241, uptime 0:01:02regiond RUNNING pid 22248, uptime 0:01:02syslog RUNNING pid 22366, uptime 0:00:45If any services are not running, follow this process to re-initialize MAAS.At this point the MAAS web GUI should be accessible from a web browser on a machine sharing the same network as the MAAS Controller.Initial Set-UpAccess the MAAS web GUI from your local browser. You should be able to access it as long as you are on the same network as the MAAS Controller and MAAS is running.Log in to the MAAS web GUI and follow the set-up process. The steps are mostly straight-forward.Import SSH KeysIn the next step of the initial set-up, you will be asked to input an ssh-key so that you can ssh into the machines MAAS deploys using RSA key authentication instead of a password. Grab the machine you intend to use to SSH into the MAAS-deployed nodes and retrieve the RSA key pair of that machine. If you are on a Linux machine and you have already generated SSH keys, the key can usually be retrieved from the default directory by entering the following command.sudo cat /home/$USERNAME/.ssh/id_rsa.pubMore than likely though, you will need to generate your “private/public key pair”. To generate the RSA key pair enter the following command. Follow the shell prompts and take note of the directory and passphrase (if any) used.ssh-keygen -t rsa -b 4096If you saved the RSA key pair to the default directory you will be able to retrieve your public key with the following command.sudo cat /home/$USERNAME/.ssh/id_rsa.pubThe output of this will look something like this:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC/2sats4R9ino4qZfNiPFL+PYAVjY6vdRP9G9KJZPyxeOhbcpUPQ88gf2TpjEhy9CICweJ9Wt71uhkyh7iL4nW2ZN1oPyF9jzJTTfxPyAGjV6cOOPC1ispOgmFu3ZklGpRXAaNYsgIpp5FGGyioIAatzDuBjHffYEEXqmj6qkamRhCZZdMl7EBxa0rwsTlOfdY72ohpqwqUQr6QFMpvnSO1VFTjQJIXSvpDBigdhYsL0TxH+4S9UG+ywBQunhaOpX34Z6bIQT1rfZbDEfMh/C6qcQQSFghclhCjvqn2V7FNkiYcIuImf18xLQMnZU2NKRM9x2inhdjWUXvpihJbVzZuZ03AAm0NF1H2va3fBMJrTAXKLf4wTj1k+841+d59EVD6eUGW94p3sAzkC39FQMizv1PovFCSFFjNt5zFJWEFjMNYJWvLmx2DUnEnn7Qsmjw8U66YcI03FxWwW1sab+hbsS9y5rjOlUIDc1zjHlfRZmqf0obAghcQ852WvRYKv6TJ/WE2RQtUXg0/lJjtG70+mnrOxqAE3Pk2kT2gZumhk01VlvZCVBz8bLNdZ3r5jjEVYJ/MChQW7Uv828EtA1fayXqXLtIrYZxCPcvA99GKN3/xT0HNGWSYdrelVf0CZ207aVjJuP2mfDr5DWZ9PhXxiiz/hF6TP1abKOqNfsb8w== controller@maasYou will need to copy this output and paste it into the MAAS web GUI.With the RSA public key imported, continue through the initial set-up. Once you land on the page below there are a couple things that still need to be configured.ConfigurationEnable DHCPThis step only applies if you’ve decided to let MAAS serve DHCP, as described in the Serving DHCP section of this guide.To enable DHCP click on the “Controllers” tab at the top, and select the controller (called maas.maas in this case).In here click on “VLANs” and click on “VLAN” “untagged”.Scroll down to the DHCP section and click on “Enable DHCP”.Now, click on “Configure DHCP”At this point DHCP is configured.Configure DNSEarly in our deployment we encountered some issues where MAAS would commission the servers but the servers would not be able to reach the internet to install APT packages. We figured out that, though we had specified the DNS during the initial set-up process, we also needed to specify the DNS within the controller VLAN itself. To set this up go to the “Controllers” tab in the MAAS web GUI, and select your MAAS controller (“maas.maas” in this case).After selecting the controller from the list, click on “VLANs” and then click on the subnet listed under “subnets” (in this case it is “192.168.1.0/24”).Now, find “Subnet summary” and click on “Edit”.Specify the appropriate DNS in the DNS text field and click “Save summary”.Disable MAAS ProxyMAAS uses a proxy to download boot images and provision packages for APT and YUM. This has caused issues in our environment where MAAS does not see the hardware inventory of our nodes and, though it manages to “enlist” the nodes, it fails to deploy them. To disable the proxy from the web GUI, go to the “Settings” tab at the top and click the “Proxy” section on the left hand side. Then select the “Don’t use a proxy” option.With DHCP handled, proxy disabled, and DNS properly configured, you are now ready to enlist nodes.Enlisting Nodes (MAAS serving DHCP)Enlisting a node when MAAS is serving DHCP is very simple. All you have to do to is power the server on - as long as it is on the same network as the MAAS Controller, and IPMI + PXE boot are properly configured (as specified in Hardware Configuration), the node will boot under MAAS instruction and will be listed in the web GUI under “Machines” as shown below. Once the node is enlisted and in “New” state, the physical machine will power down.It can be challenging to identify what physical node corresponds to each machine listed in the MAAS web GUI (especially if the nodes have similar hardware configuration), so it is best to enlist one node at a time.When enlisting each node, wait until MAAS says the node is in a “New” state. At that point click on the node and on the top left corner rename it to make it more easily identifiable in the future. In this case we are naming this node “juju” since it will be the JuJu Controller node.Next, click on the “Configuration” tab, add the appropriate tag to each node. The JuJu Controller needs to be tagged “juju” and the nodes that will be used as compute nodes need to be tagged “compute”.With nodes renamed, tagged and in “New” state, the list of Machines on the MAAS web GUI should look something like this.Now the machines are ready to be “Commissioned”. To do this select all the machines and click on the green “Take Action” button, then click on “Commission”, and “Start commissioning machines”.This process will take some time but once the machines are in a “Ready” state you can work on deploying the JuJu Controller and Compute nodes.Next Section: JuJu" }, { "title": "OpenStack Deployment - JuJu", "url": "/posts/openstack-deployment-juju/", "categories": "Training-SOC, OpenStack", "tags": "openstack, cloud, deployment, soc, guide, charms, juju, maas", "date": "2021-01-26 07:00:00 -0700", "snippet": "JuJu will have two parts to it, one is the application we interact with through command-line (which gets installed on the MAAS Controller), and the other is the JuJu Controller itself which actually “orchestrates” the commands we give JuJu through the command line. JuJu will have control of MAAS, so it can power-cycle nodes as needed when we deploy the OpenStack Charm containers later on.InstallationAs mentioned, we need to install the JuJu application on the MAAS Controller. As of the writing of this guide the latest version of JuJu that played “nicely” with OpenStack Wallaby is JuJu 2.8.10. We had some issues deploying OpenStack charms using JuJu 2.9, however this may have been patched by the time this guide is being used.Use the following command to install JuJu 2.8.10sudo snap install juju --channel=2.8/stable --classicConfigurationAt this point, it is important to note that JuJu commands should not be run as root. JuJu keeps track of permissions and owners of configuration files, and if these do not match perfectly JuJu will throw error messages.With that being said, the first step of the JuJu configuration is to create a cloud. This is done by first creating a YAML file called maas-cloud.yaml specifying the cloud configurations as follows, where “maas-cloud” is the name of the cloud (replace the IP 10.0.0.2 with that of the MAAS GUI).clouds: maas-cloud: type: maas auth-types: [oauth1] endpoint: http://10.0.0.2:5240/MAASThen use the following JuJu command to create a cloud using the configurations specified in the maas-cloud.yaml and specify the name of the cloud, in this case “maas-cloud”.juju add-cloud --client -f maas-cloud.yaml maas-cloudNow you need to follow similar steps to add MAAS credentials (API key) to the “maas-cloud” that was just created, so that JuJu can control MAAS. This is done by first creating a file called maas-cred.yaml with the following specifications, where “admin” is the name you want to assign to the user credentials (API key).credentials: maas-cloud: admin: auth-type: oauth1 maas-oauth: PCwrBEpRsbcPu9FDzT:5YrGQWLs45RpsgHD4R:JvW9ywBqCS6SDZHzVW9g6yAVNYA6BNXnThe “maas-auth” API key above is found in the “Controller” tab on the MAAS web GUI, under “API Keys”.With the maas-creds.yaml finished, we need to add the credentials to the “maas-cloud” we created in the step before. To do this use the following command.juju add-credential --client -f maas-creds.yaml maas-cloudAt this point you’ve created a MAAS cloud that JuJu can control called “maas-cloud”, and you can now deploy the JuJu Controller Node.DeploymentThere are two simple steps to deploying the JuJu Controller node.First, we need to bootstrap a node using the following command, where we are specifying that we want Ubuntu Focal, and we tell juju to only bootstrap nodes tagged “juju”. We then specify the “maas-cloud” where the node lives, and we name the controller we are creating “juju-controller”.juju bootstrap --bootstrap-series=focal --constraints tags=juju maas-cloud juju-controllerIf you followed all steps correctly, the node you tagged “juju” within MAAS will be powered on and Ubuntu 20.04 will be deployed on this node. The deployment process will take some time, you will know youre ready for the last step of the JuJu Controller deployment when the command line output looks something like this.juju bootstrap --bootstrap-series=focal --constraints tags=juju maas-cloud juju-controllerCreating Juju controller &quot;juju-controller&quot; on maas-cloudLooking for packaged Juju agent version 2.8.10 for amd64Launching controller instance(s) on maas-cloud...- 4cw3ee (arch=amd64 mem=7G cores=16) Installing Juju agent on bootstrap instance Fetching Juju Dashboard 0.6.2 Waiting for address Attempting to connect to 192.168.1.240:22 Connected to 192.168.1.240 Running machine configuration script... Bootstrap agent now started Contacting Juju controller at 192.168.1.240 to verify accessibility...Bootstrap complete, controller &quot;juju-controller&quot; is now availableController machines are in the &quot;controller&quot; modelInitial model &quot;default&quot; addedThe last step is for organization purposes. We will create what JuJu calls a “model”. All the OpenStack containers we deploy later and their relationships will live within that “model”. In this case we are calling the model “openstack”.juju add-model --config default-series=focal openstackTo check if JuJu was deployed correctly input the JuJu status command.juju statusThe output should look like this.juju statusModel Controller Cloud/Region Version SLA Timestampopenstack juju-controller maas-cloud 2.8.10 unsupported 02:09:10ZModel &quot;admin/openstack&quot; is empty.With the model created, you are now ready to begin deploying OpenStack.Next Section: Charms" }, { "title": "OpenStack Deployment - Charms", "url": "/posts/openstack-deployment-charms/", "categories": "Training-SOC, OpenStack", "tags": "openstack, cloud, deployment, soc, guide, charms, juju, maas", "date": "2021-01-25 07:00:00 -0700", "snippet": "We will be using JuJu to orchestrate the OpenStack deployment one individual “charm” container at a time. OpenStack can also be deployed via “bundles” - for instructions on a bundle deployment follow the official guide on the OpenStack website. “There are many moving parts involved in a charmed OpenStack install. During much of the process there will be components that have not yet been satisfied, which will cause error-like messages to be displayed in the output of the juju status command. Do not be alarmed. Indeed, these are opportunities to learn about the interdependencies of the various pieces of software. Messages such as Missing relation and blocked will vanish once the appropriate applications and relations have been added and processed.”InstallationFirst, make sure you are interacting with the correct juju controller and model.juju switch juju-controller:openstackWith that out of the way, it may be useful to monitor the deployment progress either through the JuJu dashboard or using the JuJu status command.Monitoring the OpenStack DeploymentTo access the JuJu dashboard, enter the following command.juju dashboardThe output will look something like this.juju dashboard Dashboard 0.6.2 for controller &quot;juju-controller&quot; is enabled at: https://192.168.1.240:17070/dashboard Your login credential is: username: admin password: 536d568fb04609daee40c61abc36844dAlternatively, to monitor JuJu status use this command.watch -n 1 -c juju status --colorCeph-OSDThe first charm we will deploy is the Ceph-osd charm or “object storage device”. This is done by first creating a YAML file called ceph-osd.yaml which specifies the charm’s configurations, where “/dev/sdb” is the path to the sdbdrive.ceph-osd: osd-devices: /dev/sdb source: cloud:focal-wallabyNext, we will deploy the Ceph-osd charm to 4 nodes. Since we only have compute nodes, the “compute” tag constraint is not necessary strictly speaking, but it is good practice for future deployments.juju deploy -n 4 --config ceph-osd.yaml --constraints tags=compute ceph-osdWith this being the first charm deployed to the compute nodes, MAAS will first power on the servers and deploy Ubuntu 20.04 to each node - and once that is done, JuJu will proceed to install the charm software. You can monitor the progress using the JuJu status command output - it will look like this once the Ceph-osd charm is finished installing.Model Controller Cloud/Region Version SLA Timestampopenstack juju-controller maas-cloud 2.8.10 unsupported 05:09:54ZApp Version Status Scale Charm Store Rev OS Notesceph-osd 16.2.0 blocked 4 ceph-osd jujucharms 310 ubuntuUnit Workload Agent Machine Public address Ports Messageceph-osd/0 blocked idle 0 192.168.1.241 Missing relation: monitorceph-osd/1* blocked idle 1 192.168.1.235 Missing relation: monitorceph-osd/2 blocked idle 2 192.168.1.236 Missing relation: monitorceph-osd/3 blocked idle 3 192.168.1.237 Missing relation: monitorMachine State DNS Inst id Series AZ Message0 started 192.168.1.241 node0 focal default Deployed1 started 192.168.1.235 node1 focal default Deployed2 started 192.168.1.236 node2 focal default Deployed3 started 192.168.1.237 node3 focal default DeployedNova ComputeWith the Ceph-osd installed, the Nova Compute charm is next on the list of deployment. The process is very similar across charms. First, create the file nova-compute.yaml.nova-compute: config-flags: default_ephemeral_format=ext4 enable-live-migration: true enable-resize: true migration-auth-type: ssh openstack-origin: cloud:focal-wallabyThen, deploy the Nova Compute charm using the file nova-compute.yaml configurations.juju deploy -n 3 --to 1,2,3 --config nova-compute.yaml nova-computeMySQL InnoDB Clusterjuju deploy -n 3 --to lxd:0,lxd:1,lxd:2 mysql-innodb-clusterVaultjuju deploy --to lxd:3 vaultjuju deploy mysql-router vault-mysql-routerjuju add-relation vault-mysql-router:db-router mysql-innodb-cluster:db-routerjuju add-relation vault-mysql-router:shared-db vault:shared-dbUnseal VaultTo unseal the Vault container, first install the vault agent application on the MAAS Controller.sudo snap install vaultWith the Vault application installed, run juju status to obtain the IP of the vault container. Hint: The port specified on juju status for this IP is 8200. This step is to tell the Vault application where to reach the Vault container.export VAULT_ADDR=&quot;http://10.0.0.126:8200&quot;Now ask the vault for 5 keys.vault operator init -key-shares=5 -key-threshold=3Sample output:Unseal Key 1: XONSc5Ku8HJu+ix/zbzWhMvDTiPpwWX0W1X/e/J1XixvUnseal Key 2: J/fQCPvDeMFJT3WprfPy17gwvyPxcvf+GV751fTHUoN/Unseal Key 3: +bRfX5HMISegsODqNZxvNcupQp/kYQuhsQ2XA+GamjY4Unseal Key 4: FMRTPJwzykgXFQOl2XTupw2lfgLOXbbIep9wgi9jQ2lsUnseal Key 5: 7rrxiIVQQWbDTJPMsqrZDKftD6JxJi6vFOlyC0KSabDBInitial Root Token: s.ezlJjFw8ZDZO6KbkAkm605QvVault initialized with 5 key shares and a key threshold of 3. Please securelydistribute the key shares printed above. When the Vault is re-sealed,restarted, or stopped, you must supply at least 3 of these keys to unseal itbefore it can start servicing requests.Vault does not store the generated master key. Without at least 3 key toreconstruct the master key, Vault will remain permanently sealed!It is possible to generate new unseal keys, provided you have a quorum ofexisting unseal keys shares. See &quot;vault operator rekey&quot; for more information.Now run vault operator unseal $Key and replace $Key with each of the 5 keys above.vault operator unseal XONSc5Ku8HJu+ix/zbzWhMvDTiPpwWX0W1X/e/J1Xixvvault operator unseal J/fQCPvDeMFJT3WprfPy17gwvyPxcvf+GV751fTHUoN/vault operator unseal +bRfX5HMISegsODqNZxvNcupQp/kYQuhsQ2XA+GamjY4vault operator unseal FMRTPJwzykgXFQOl2XTupw2lfgLOXbbIep9wgi9jQ2lsvault operator unseal 7rrxiIVQQWbDTJPMsqrZDKftD6JxJi6vFOlyC0KSabDBNow grab the token (line 7 in the sample output above), and enter these commands: Note: Replace the token below s.ezlJjFw8ZDZO6KbkAkm605Qv with your own.export VAULT_TOKEN=s.ezlJjFw8ZDZO6KbkAkm605Qvvault token create -ttl=10mSample output:Key Value--- -----token s.QMhaOED3UGQ4MeH3fmGOpNEDtoken_accessor nApB972Dp2lnTTIF5VXQqnnbtoken_duration 10mtoken_renewable truetoken_policies [&quot;root&quot;]identity_policies []policies [&quot;root&quot;]Now grab the last token (line 3 of the sample output above), and enter this command: Note: Replace the token below s.QMhaOED3UGQ4MeH3fmGOpNED with your own.juju run-action --wait vault/leader authorize-charm token=s.QMhaOED3UGQ4MeH3fmGOpNEDGenerate the Certificate Authority (CA)The last step to unseal the vault is to generate a CA using the command below.juju run-action --wait vault/leader generate-root-caAt this point the Vault is ready, and you can continue with the charm deployment.juju add-relation mysql-innodb-cluster:certificates vault:certificatesNeutron networkingCreate a file called neutron.yaml with the YAML below. Note: The NIC in this case is called eth1 (line2 below) - but more than likely this will be different for our environment. Tip: To find out the name of the NIC, go in the MAAS web GUI and click on ‘machines’ at the top. Now click on one of the ‘compute’ nodes and click on ‘Networking’, this will give you a list of the NIC ports and what they are called. There is one NIC that has a green checkmark saying it is used for PXE booting. We need one that says ‘Unconfigured’. If this ‘unconfigured’ NIC is called “eno2”, you would replace “eth1” on the YAML below with “eno2” (Example: bridge-interface-mappings: br-ex:eno2).ovn-chassis: bridge-interface-mappings: br-ex:eth1 ovn-bridge-mappings: physnet1:br-exneutron-api: neutron-security-groups: true flat-network-providers: physnet1 worker-multiplier: 0.25 openstack-origin: cloud:focal-wallabyovn-central: source: cloud:focal-wallabyjuju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --config neutron.yaml ovn-centraljuju deploy --to lxd:1 --config neutron.yaml neutron-apijuju deploy neutron-api-plugin-ovnjuju deploy --config neutron.yaml ovn-chassisjuju add-relation neutron-api-plugin-ovn:neutron-plugin neutron-api:neutron-plugin-api-subordinatejuju add-relation neutron-api-plugin-ovn:ovsdb-cms ovn-central:ovsdb-cmsjuju add-relation ovn-chassis:ovsdb ovn-central:ovsdbjuju add-relation ovn-chassis:nova-compute nova-compute:neutron-pluginjuju add-relation neutron-api:certificates vault:certificatesjuju add-relation neutron-api-plugin-ovn:certificates vault:certificatesjuju add-relation ovn-central:certificates vault:certificatesjuju add-relation ovn-chassis:certificates vault:certificatesjuju deploy mysql-router neutron-api-mysql-routerjuju add-relation neutron-api-mysql-router:db-router mysql-innodb-cluster:db-routerjuju add-relation neutron-api-mysql-router:shared-db neutron-api:shared-dbKeystoneCreate a file called keystone.yaml with the YAML below.keystone: worker-multiplier: 0.25 openstack-origin: cloud:focal-wallabyjuju deploy --to lxd:0 --config keystone.yaml keystonejuju deploy mysql-router keystone-mysql-routerjuju add-relation keystone-mysql-router:db-router mysql-innodb-cluster:db-routerjuju add-relation keystone-mysql-router:shared-db keystone:shared-dbjuju add-relation keystone:identity-service neutron-api:identity-servicejuju add-relation keystone:certificates vault:certificatesRabbitMQjuju deploy --to lxd:2 rabbitmq-serverjuju add-relation rabbitmq-server:amqp neutron-api:amqpjuju add-relation rabbitmq-server:amqp nova-compute:amqp[SAMPLE JUJU STATUS OUTPUT GOES HERE]Nova cloud controllerCreate a file called nova-cloud-controller.yaml with the YAML below.nova-cloud-controller: network-manager: Neutron worker-multiplier: 0.25 openstack-origin: cloud:focal-wallabyjuju deploy --to lxd:3 --config nova-cloud-controller.yaml nova-cloud-controllerjuju deploy mysql-router ncc-mysql-routerjuju add-relation ncc-mysql-router:db-router mysql-innodb-cluster:db-routerjuju add-relation ncc-mysql-router:shared-db nova-cloud-controller:shared-dbjuju add-relation nova-cloud-controller:identity-service keystone:identity-servicejuju add-relation nova-cloud-controller:amqp rabbitmq-server:amqpjuju add-relation nova-cloud-controller:neutron-api neutron-api:neutron-apijuju add-relation nova-cloud-controller:cloud-compute nova-compute:cloud-computejuju add-relation nova-cloud-controller:certificates vault:certificatesPlacementCreate a file called placement.yaml with the YAML below.placement: worker-multiplier: 0.25 openstack-origin: cloud:focal-wallabyjuju deploy --to lxd:3 --config placement.yaml placementjuju deploy mysql-router placement-mysql-routerjuju add-relation placement-mysql-router:db-router mysql-innodb-cluster:db-routerjuju add-relation placement-mysql-router:shared-db placement:shared-dbjuju add-relation placement:identity-service keystone:identity-servicejuju add-relation placement:placement nova-cloud-controller:placementjuju add-relation placement:certificates vault:certificatesOpenStack dashboardjuju deploy --to lxd:2 --config openstack-origin=cloud:focal-wallaby openstack-dashboardjuju deploy mysql-router dashboard-mysql-routerjuju add-relation dashboard-mysql-router:db-router mysql-innodb-cluster:db-routerjuju add-relation dashboard-mysql-router:shared-db openstack-dashboard:shared-dbjuju add-relation openstack-dashboard:identity-service keystone:identity-servicejuju add-relation openstack-dashboard:certificates vault:certificatesGlanceCreate a file called glance.yaml with the YAML below.glance: worker-multiplier: 0.25 openstack-origin: cloud:focal-wallabyjuju deploy --to lxd:3 --config glance.yaml glancejuju deploy mysql-router glance-mysql-routerjuju add-relation glance-mysql-router:db-router mysql-innodb-cluster:db-routerjuju add-relation glance-mysql-router:shared-db glance:shared-dbjuju add-relation glance:image-service nova-cloud-controller:image-servicejuju add-relation glance:image-service nova-compute:image-servicejuju add-relation glance:identity-service keystone:identity-servicejuju add-relation glance:certificates vault:certificates[SAMPLE JUJU STATUS OUTPUT GOES HERE]Ceph monitorCreate a file called ceph-mon.yaml with the YAML below.ceph-mon: expected-osd-count: 3 monitor-count: 3 source: cloud:focal-wallabyjuju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --config ceph-mon.yaml ceph-monjuju add-relation ceph-mon:osd ceph-osd:monjuju add-relation ceph-mon:client nova-compute:cephjuju add-relation ceph-mon:client glance:cephCinderCreate a file called cinder.yaml with the YAML below.cinder: block-device: None glance-api-version: 2 worker-multiplier: 0.25 openstack-origin: cloud:focal-wallabyjuju deploy --to lxd:1 --config cinder.yaml cinderjuju deploy mysql-router cinder-mysql-routerjuju add-relation cinder-mysql-router:db-router mysql-innodb-cluster:db-routerjuju add-relation cinder-mysql-router:shared-db cinder:shared-dbjuju add-relation cinder:cinder-volume-service nova-cloud-controller:cinder-volume-servicejuju add-relation cinder:identity-service keystone:identity-servicejuju add-relation cinder:amqp rabbitmq-server:amqpjuju add-relation cinder:image-service glance:image-servicejuju add-relation cinder:certificates vault:certificatesjuju deploy cinder-cephjuju add-relation cinder-ceph:storage-backend cinder:storage-backendjuju add-relation cinder-ceph:ceph ceph-mon:clientjuju add-relation cinder-ceph:ceph-access nova-compute:ceph-accessCeph RADOS Gatewayjuju deploy --to lxd:0 --config source=cloud:focal-wallaby ceph-radosgwjuju add-relation ceph-radosgw:mon ceph-mon:radosgwNTPjuju deploy ntpjuju add-relation ceph-osd:juju-info ntp:juju-infoFinal results and dashboard accessOnce the juju status output has settled it should look something like this:[SAMPLE JUJU STATUS OUTPUT GOES HERE]At this point you should have a fully deployed OpenStack. To retrieve the OpenStack Dashboard IP enter the following command:juju status --format=yaml openstack-dashboard | grep public-address | awk &#39;{print $2}&#39; | head -1And to retrieve the password use:juju run --unit keystone/leader leader-get admin_passwdMake Accessing the Openstack Dashboard easierIt is sometimes helpful to have aliases for commonly used commands.To create an alias that outputs the URL and password of the OpenStack Dashbaord, use your favorite text editor to edit the .bashrc file.vim ~/.bashrcWith the .bashrc file open, scroll down to where you see aliases listed. Here enter the following aliases in new lines:alias openstack-ip=&#39;juju status --format=yaml openstack-dashboard | grep public-address | awk &#39;&quot;&#39;&quot;&#39;{print $2}&#39;&quot;&#39;&quot;&#39; | head -1&#39;alias openstack-pass=&#39;juju run --unit keystone/leader leader-get admin_passwd&#39;alias openstack-login=&#39;openstackip=$(openstack-ip) &amp;amp;&amp;amp; echo &quot;Dashboard URL: https://&quot;$openstackip&quot;/horizon&quot; &amp;amp;&amp;amp; echo &quot;Username: admin&quot; &amp;amp;&amp;amp; openstackpass=$(openstack-pass) &amp;amp;&amp;amp; echo &quot;Passowrd: &quot;$openstackpass &amp;amp;&amp;amp; echo &quot;Domain: admin_domain&quot;&#39;Now save your edits, and exit the file. To make the new changes effective source the .bashrc file.source ~/.bashrcIf you followed the steps correctly the command openstack-login should output your Dashboard URL, username, password, and domain.VM consolesjuju config nova-cloud-controller console-access-protocol=novncNext Section: Configuration" }, { "title": "OpenStack Deployment - Configuration", "url": "/posts/openstack-deployment-configuration/", "categories": "Training-SOC, OpenStack", "tags": "openstack, cloud, deployment, soc, guide, charms, juju, maas", "date": "2021-01-24 07:00:00 -0700", "snippet": "Next Section: Management" }, { "title": "OpenStack Deployment - Iso to Image", "url": "/posts/openstack-deployment-Iso_to_Image/", "categories": "Training-SOC, OpenStack", "tags": "openstack, cloud, deployment, soc, guide, instance, image, volume", "date": "2021-01-24 07:00:00 -0700", "snippet": "Installing an ISO on a Volume Download an ISO version of an OS e.g Ubuntu_Focal_Desktop.iso Upload this image to glance in openstack using the openstack dashboard, by navigating to the images tab under compute, then selecting Create Image. Create a new volume, ensuring the size is large enough to install the OS. (E.g: 25 GB for ubuntu desktop). Instantiate a new instance with the ISO intended to be installed. Attatch the newly created volume to the instance. This can be done in the instances section, using the right drop down menu under actions and selecting ‘attatch volume’. Switch to the console view of the instance and proceed to install the OS with the destination being the newly attatched volume. After installing the OS to the volume, delete the instance.Converting the Volume to an ImageNavigate to the Volumes section and use the right drop down menu to select upload as image. This newly created image can be used to create new instances with the OS installed. Ensure to re-partition the OS to utilize the larger instance size. (For Ubuntu Desktop the utility program works well)" }, { "title": "OpenStack Deployment - Management", "url": "/posts/openstack-deployment-management/", "categories": "Training-SOC, OpenStack", "tags": "openstack, cloud, deployment, soc, guide, charms, juju, maas", "date": "2021-01-23 07:00:00 -0700", "snippet": "Next Section: Debugging" }, { "title": "OpenStack Deployment - Debugging", "url": "/posts/openstack-deployment-debugging/", "categories": "Training-SOC, OpenStack", "tags": "openstack, cloud, deployment, soc, guide, charms, juju, maas", "date": "2021-01-22 07:00:00 -0700", "snippet": "" } ]
